{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# naive LSTM model trained with smooth data\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.lstm_1 = nn.LSTM(self.input_size, self.hidden_size)\n",
    "        self.lstm_2 = nn.LSTM(self.hidden_size, self.hidden_size)\n",
    "        self.lstm_3 = nn.LSTM(self.hidden_size, self.hidden_size)\n",
    "        self.lstm_4 = nn.LSTM(self.hidden_size, self.hidden_size)\n",
    "        self.lstm_5 = nn.LSTM(self.hidden_size, self.hidden_size)\n",
    "        self.lstm_6 = nn.LSTM(self.hidden_size, self.hidden_size)\n",
    "        self.lstm_7 = nn.LSTM(self.hidden_size, self.hidden_size)\n",
    "        \n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "          \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, ch_1, ch_2, ch_3, ch_4, ch_5, ch_6, ch_7):\n",
    "        (hidden_1, cell_1), (hidden_2, cell_2), (hidden_3, cell_3), (hidden_4, cell_4), (hidden_5, cell_5) = ch_1, ch_2, ch_3, ch_4, ch_5\n",
    "        (hidden_6, cell_6), (hidden_7, cell_7) = ch_6, ch_7\n",
    "        \n",
    "        \n",
    "        output, (hidden_1, cell_1) = self.lstm_1(input.view(1,1,-1).float(), (hidden_1, cell_1))\n",
    "        output_1 = output\n",
    "        \n",
    "        output, (hidden_2, cell_2) = self.lstm_2(output, (hidden_2, cell_2))\n",
    "        output_2 = output\n",
    "        \n",
    "        output, (hidden_3, cell_3) = self.lstm_3(output + output_1, (hidden_3, cell_3)) # skip_connection 1\n",
    "        output_3 = output\n",
    "        \n",
    "        output, (hidden_4, cell_4) = self.lstm_4(output + output_2, (hidden_4, cell_4)) # skip_connection 2\n",
    "        output_4 = output\n",
    "        \n",
    "        output, (hidden_5, cell_5) = self.lstm_5(output + output_3, (hidden_5, cell_5)) # skip_connection 3\n",
    "        output_5 = output\n",
    "        \n",
    "        output, (hidden_6, cell_6) = self.lstm_6(output + output_4, (hidden_6, cell_6)) # skip_connection 4\n",
    "        \n",
    "        output, (hidden_7, cell_7) = self.lstm_7(output + output_5, (hidden_7, cell_7)) # skip_connection 5\n",
    "        \n",
    "        output = self.out(output[0])\n",
    "        #output = self.softmax(output)\n",
    "        return output, (hidden_1, cell_1),(hidden_2, cell_2),(hidden_3, cell_3),(hidden_4, cell_4),(hidden_5, cell_5),(hidden_6, cell_6),(hidden_7, cell_7)\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.rand((1, 1, self.hidden_size), device=device)/100\n",
    "    \n",
    "    def init_cell(self):\n",
    "        return torch.rand((1, 1, self.hidden_size), device=device)/100\n",
    "    \n",
    "    \n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/yiqin/2018summer_project/smooth_data.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2c9c5b2d3a87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \"\"\"\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/yiqin/2018summer_project/smooth_data.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtrain_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/yiqin/2018summer_project/smooth_data.pkl'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# load data from file\n",
    "\"\"\"with open(\"/home/yiqin/2018summer_project/DeepMusic/pitch_data.pkl\", \"rb\") as f:\n",
    "    dic = pickle.load(f)\n",
    "    train_X = dic[\"X\"]\n",
    "    #train_Y = dic[\"Y\"]\n",
    "    #time_X = dic[\"time\"]\n",
    "    \"\"\"\n",
    "    \n",
    "with open(\"/home/yiqin/2018summer_project/datsmooth_data.pkl\", \"rb\") as f:\n",
    "    dic = pickle.load(f)\n",
    "    train_X = dic[\"X\"]\n",
    "    train_Y = dic[\"Y\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "target_Tensor = train_Y\n",
    "maximum_target = len(train_Y)\n",
    "\n",
    "def focal_loss(gamma, rescale, criterion, output, target):\n",
    "    if int(target) == 1:\n",
    "        p_negative = (1 - output[0,1])**gamma\n",
    "        loss = rescale * p_negative * criterion(output, target.unsqueeze(0).long())\n",
    "    else:\n",
    "        p_negative = (1 - output[0,0])**gamma\n",
    "        loss = p_negative * criterion(output, target.unsqueeze(0).long())\n",
    "    return loss\n",
    "\n",
    "\n",
    "import random\n",
    "teacher_forcing_ratio = 1\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, decoder, decoder_optimizer, criterion, verbose = False):\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    hidden_1 = decoder.init_hidden()\n",
    "    hidden_2 = decoder.init_hidden()\n",
    "    hidden_3 = decoder.init_hidden()\n",
    "    hidden_4 = decoder.init_hidden()\n",
    "    hidden_5 = decoder.init_hidden()\n",
    "    hidden_6 = decoder.init_hidden()\n",
    "    hidden_7 = decoder.init_hidden()\n",
    "    cell_1 = decoder.init_cell()\n",
    "    cell_2 = decoder.init_cell()\n",
    "    cell_3 = decoder.init_cell()\n",
    "    cell_4 = decoder.init_cell()\n",
    "    cell_5 = decoder.init_cell()\n",
    "    cell_6 = decoder.init_cell()\n",
    "    cell_7 = decoder.init_cell()\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    temp = []\n",
    "    temp_score = []\n",
    "    \n",
    "    decoder_input = input_tensor[0]\n",
    "    \n",
    "    for di in range(0, target_length):\n",
    "        decoder_output, (hidden_1, cell_1), (hidden_2, cell_2), (hidden_3, cell_3), (hidden_4, cell_4), (hidden_5, cell_5), (hidden_6, cell_6), (hidden_7, cell_7) = decoder(decoder_input, \n",
    "                        (hidden_1, cell_1), (hidden_2, cell_2), (hidden_3, cell_3), (hidden_4, cell_4), (hidden_5, cell_5), (hidden_6, cell_6), (hidden_7, cell_7))\n",
    "        if verbose:\n",
    "            output = float(decoder_output.data.cpu().numpy())\n",
    "            temp.append(str('%.4f'%output))\n",
    "            #temp_score.append(decoder_output)\n",
    "\n",
    "        loss += criterion(decoder_output.squeeze(0), target_tensor[di].float())\n",
    "\n",
    "        if di + 1 < target_length:\n",
    "            decoder_input = input_tensor[di + 1]\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Prediction :\", temp) \n",
    "        print(\"Target:\", target_tensor.squeeze())\n",
    "        #print(\"Score :\", temp_score)\n",
    "        \n",
    "    \n",
    "\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "\n",
    "# In[63]:\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "# In[64]:\n",
    "\n",
    "def trainIters(decoder, n_iters, print_every = 1000, plot_every = 100, learning_rate = 0.01, CEL_weight=[1,5], total_batch = maximum_target, gamma = 0.1):    \n",
    "    start = time.time()\n",
    "    \n",
    "    plot_losses = []\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "    \n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr = learning_rate)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.StepLR(decoder_optimizer, step_size = total_batch, gamma = gamma)\n",
    "    \n",
    "    \n",
    "    for iter in range(1, n_iters + 1):\n",
    "        num = iter % total_batch\n",
    "        verbose = (iter % print_every == 0)\n",
    "        input_tensor = train_X[num].to(device)\n",
    "        target_tensor = target_Tensor[num].to(device)\n",
    "        input_tensor = Variable(input_tensor, requires_grad = True)\n",
    "        #print(input_tensor.shape, target_tensor.shape)\n",
    "        if input_tensor.shape[0]<2:\n",
    "            continue\n",
    "        if input_tensor.shape[0] != target_tensor.shape[0]:\n",
    "            continue\n",
    "        \n",
    "        loss = train(input_tensor, target_tensor, decoder, \n",
    "                     decoder_optimizer, criterion, verbose = verbose)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * print_every, print_loss_avg))\n",
    "            torch.save(decoder.state_dict(), 'naive_lstm_train.pt')\n",
    "        \n",
    "        scheduler.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "hidden_size = 256\n",
    "output_size = 1\n",
    "\n",
    "decoder = DecoderRNN(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "trainIters(decoder, 100000, print_every=1000, learning_rate=1e-2, gamma=0.2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
